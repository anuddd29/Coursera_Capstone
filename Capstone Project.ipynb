{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Data Acquistion\nThe first phase of the project is to acquire all of the data that is needed for this project. The initial data required can be broken down into three separate data sets:\n\nThe FourSquare Top 30 Venues to Visit in Chicago\nFor each of the Top Site get a list of up restaurants in the surrounding area\nThe Chicago Police Department Crime Data for the last Year\nImport Libraries\nIn this section we import the libraries that will be required to process the data.\n\nThe first library is Pandas.\nPandas is an open source, BSD-licensed library, providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Pandas will be used to house each of the data sets.\n\nThe second library is Requests. Requests is a Python HTTP library, released under the Apache2 License. The goal of the project is to make HTTP requests simpler and more human-friendly.\n\nThe next library in BeautifulSoup Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n"}, {"metadata": {}, "cell_type": "code", "source": " Import Pandas to provide DataFrame support\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Import Requests\nimport requests\n\n# Import BeautifulSoup\nfrom bs4 import BeautifulSoup", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import yaml\n\nwith open(\"./capstone_data/config.yaml\", \"r\") as f:\n    cfg = yaml.load(f)\n \nsearch_params = {\n    'client_id': cfg['client_id'],\n    'client_secret': cfg['client_secret'],\n    'intent': 'browse',\n    'limit': 50,\n    'v': cfg['version']\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nFourSquare Top 30 Venues to Visit in Chicago\nFourSquare does not actually provide an API that will return a list of the top venues to visit in a city. To get this list we can though use the FourSquare website directly to request the top sites in Chicago and then use BeautifulSoup to scrape the data we need. Once we have this starting data the other supplemental data we need to complete this dataset can be retrieved from using the FourSquare Venue API."}, {"metadata": {}, "cell_type": "code", "source": "# Use the Requests get method to request the top sites in Chicago\npage = requests.get(\n    \"https://foursquare.com/explore?mode=url&near=Chicago%2C%20IL%2C%20United%20States&nearGeoId=72057594042815334&q=Top%20Picks\")\n\n# Convert the HTML response into a BeautifulSoup Object\nsoup = BeautifulSoup(page.content, 'html.parser')\n\n# Use the BeautifulSoup find_all method to extract each top site venue details.\ntop_venues = soup.find_all('div', class_='venueDetails')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Create Top Venues Dataframe\nThe top_venues list, a sample of which is shown above, only contains some of the data required. In addition to the attributes extracted directly from the HTML code the following attributes are also required:\n\nVenue Address\nVenue Postalcode\nVenue City\nVenue Latitude\nVenue Longitude\nThese attributes will be obtained directly from FourSquare using the venues API. The process is as follows:\n\nCreate a new empty Pandas dataframe to hold the data for the Top Sites / Venues\nExtract the available attributes from the HTML code\nFor each venue\nContruct a URL to interagate the FourSquare Venue API for each top site\nUsing the venues API and the URL request the data from FourSquare\nGet the properly formatted address and the latitude and longitude data from the returned JSON\nWrite the data for each venue to the top venues dataframs", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# The column names for the top venues dataframe\nvenue_columns = ['id', \n                 'score', \n                 'category', \n                 'name', \n                 'address',\n                 'postalcode',\n                 'city',\n                 'href', \n                 'latitude', \n                 'longitude']\n\n# Create the empty top venues dataframe\ndf_top_venues = pd.DataFrame(columns=venue_columns)\n\n# For each venue in the BeautifulSoup HTML object\nfor venue in top_venues:\n    \n    # Extract the available attributes\n    venue_name = venue.find(target=\"_blank\").get_text()\n    venue_score = venue.find(class_=\"venueScore positive\").get_text()\n    venue_cat = venue.find(class_=\"categoryName\").get_text()\n    venue_href = venue.find(class_=\"venueName\").h2.a['href']\n    venue_id = venue_href.split('/')[-1]\n\n    if 'promotedTipId' in venue_id: \n        continue\n        \n    # Contruct the FourSquare venue API URL\n    url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(\n        venue_id, \n        cfg['client_id'],\n        cfg['client_secret'],\n        cfg['version'])\n    \n    # Request the venue data\n    result = requests.get(url).json()\n        \n    # Get the properly formatted address and the latitude and longitude\n    venue_address = result['response']['venue']['location']['address']\n    venue_postalcode = result['response']['venue']['location']['postalCode']\n    venue_city = result['response']['venue']['location']['city']\n    venue_latitude = result['response']['venue']['location']['lat']\n    venue_longitude = result['response']['venue']['location']['lng']\n    \n    # Add the venue to the top venues dataframe\n    df_top_venues = df_top_venues.append({'id': venue_id,\n                                          'score': venue_score,\n                                          'category': venue_cat,\n                                          'name': venue_name,\n                                          'address': venue_address,\n                                          'postalcode': venue_postalcode,\n                                          'city': venue_city,\n                                          'href': venue_href,\n                                          'latitude': venue_latitude,\n                                          'longitude': venue_longitude}, ignore_index=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Verify the shape of the top venues dataframe\ndf_top_venues.shape\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Verify the dtypes of the top venues dataframe\ndf_top_venues.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# The score type needs to be converted to float\ndf_top_venues['score'] = pd.to_numeric(df_top_venues['score'], errors='coerce').fillna(0)\n\n# Describe the score to see if there is nuch variance in the values\ndf_top_venues.score.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Review the head of the dataframe to make sure it looks as expected\ndf_top_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Finally write the dataframe to a pickle file for restoring later\ndf_top_venues.to_pickle('./capstone_pickles/top_venues.pkl')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nFourSquare Restaurent Recommendations Data\nUsing the the list of all venue id values in the Top Sites DataFrame and the FourSquare categoryID that represents all food venues, we now search for restaurants within a 500 meter radius.\n\nThe requests returns a JSON object which can then be queried for the restaurant details required. A sample restaurnt from the results returned is shown below:\n\n{  \n    \"referralId\":\"v-1538424503\",\n    \"hasPerk\":\"False\",\n    \"venuePage\":{  \n        \"id\":\"135548807\"\n    },\n    \"id\":\"55669b9b498ee34e5249ea61\",\n    \"location\":{  \n        \"labeledLatLngs\":[  \n            {  \n                \"label\":\"display\",\n                \"lng\":-87.62460021795313,\n                \"lat\":41.88169538551873\n            }\n        ],\n        \"crossStreet\":\"btwn E Madison & E Monroe St\",\n        \"postalCode\":\"60603\",\n        \"formattedAddress\":[  \n            \"12 S Michigan Ave (btwn E Madison & E Monroe St)\",\n            \"Chicago, IL 60603\",\n            \"United States\"\n        ],\n        \"distance\":155,\n        \"city\":\"Chicago\",\n        \"lng\":-87.62460021795313,\n        \"neighborhood\":\"The Loop\",\n        \"cc\":\"US\",\n        \"state\":\"IL\",\n        \"address\":\"12 S Michigan Ave\",\n        \"lat\":41.88169538551873,\n        \"country\":\"United States\"\n    },\n    \"name\":\"Cindy's\",\n    \"categories\":[  \n        {  \n            \"pluralName\":\"Gastropubs\",\n            \"id\":\"4bf58dd8d48988d155941735\",\n            \"name\":\"Gastropub\",\n            \"primary\":\"True\",\n            \"icon\":{  \n                \"prefix\":\"https://ss3.4sqi.net/img/categories_v2/food/gastropub_\",\n                \"suffix\":\".png\"\n            },\n            \"shortName\":\"Gastropub\"\n        }\n    ]\n}\nFrom this JSON the following attributes are extraced and added to the Dataframe:\n\nRestaurant ID\nRestaurant Category Name\nRestaurant Category ID\nRestaurant Nest_name\nRestaurant Address\nRestaurant Postalcode\nRestaurant City\nRestaurant Latitude\nRestaurant Longitude\nVenue Name\nVenue Latitude\nVenue Longitude\nThe only piece of data that is missing is the Score or Rating of the Restaurant. To get this we need to make another FourSquare API query using the id of the Restaurant.\n\nUsing just the data in this DataFrame we will be able to generate maps displaying the chosen Top List Venue and the best scored surrounding restaurants."}, {"metadata": {}, "cell_type": "code", "source": "# The column names for the restaurants dataframe\nrestaurants_columns = ['id',\n                       'score', \n                       'category', \n                       'categoryID', \n                       'name', \n                       'address',\n                       'postalcode',\n                       'city',\n                       'latitude',\n                       'longitude', \n                       'venue_name', \n                       'venue_latitude',\n                       'venue_longitude']\n\n# Create the empty top venues dataframe\ndf_restaurant = pd.DataFrame(columns=restaurants_columns)\n\n# Create a list of all the top venue latitude and longitude\ntop_venue_lats = df_top_venues['latitude'].values\ntop_venue_lngs = df_top_venues['longitude'].values\n\n# Create a list of all the top venue names\ntop_venue_names = df_top_venues['name'].values\n\n# Iterate over each of the top venues\n# The venue name, latitude and longitude are passed to the loop\nfor ven_name, ven_lat, ven_long in zip(top_venue_names, top_venue_lats, top_venue_lngs):\n    \n    # Configure additional Search parameters\n    # This is the FourSquare Category Id for all food venues\n    categoryId = '4d4b7105d754a06374d81259'\n    radius = 500\n    limit = 50\n    \n    # Contruct the FourSquare search API URL\n    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&categoryId={}&radius={}&limit={}'.format(\n        cfg['client_id'],\n        cfg['client_secret'],\n        ven_lat,\n        ven_long,\n        cfg['version'],\n        categoryId,\n        radius,\n        limit)\n    \n    # Make the search request\n    results = requests.get(url).json()\n    \n    # Want a good selection of Restaurents\n    # If less than 10 are returned ignore\n    if len(results['response']['venues']) < 10:\n        continue\n        \n    # Populate the new dataframe with the list of restaurants\n    # Get the values for each Restaurant from the JSON\n    for restaurant in results['response']['venues']:\n \n        # Sometimes the Venue JSON is missing data. If so ignore and continue\n        try:\n            # Get location details\n            rest_id = restaurant['id']\n            rest_category = restaurant['categories'][0]['pluralName']\n            rest_categoryID = restaurant['categories'][0]['id']\n            rest_name = restaurant['name']\n            rest_address = restaurant['location']['address']\n            rest_postalcode = restaurant['location']['postalCode']\n            rest_city = restaurant['location']['city']\n            rest_latitude = restaurant['location']['lat']\n            rest_longitude = restaurant['location']['lng']\n            \n            # Contruct the FourSquare venue API URL to get the venues rating / score\n            rest_url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(\n                rest_id, \n                cfg['client_id'],\n                cfg['client_secret'],\n                cfg['version'])\n\n            # Get the restaurant score and href\n            result = requests.get(rest_url).json()\n            rest_score = result['response']['venue']['rating']\n            \n            # Add the restaurant details to the dataframe\n            df_restaurant = df_restaurant.append({'id': rest_id,\n                                                  'score': rest_score,\n                                                  'category': rest_category,\n                                                  'categoryID': rest_categoryID,\n                                                  'name': rest_name,\n                                                  'address': rest_address,\n                                                  'postalcode': rest_postalcode,\n                                                  'city': rest_city,\n                                                  'latitude': rest_latitude,\n                                                  'longitude': rest_longitude,\n                                                  'venue_name': ven_name,\n                                                  'venue_latitude': ven_lat,\n                                                  'venue_longitude': ven_long}, ignore_index=True)\n            \n        # If there are any issue with a restaurant ignore and continue\n        except:\n            continue", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Verify the shape of the restaurants dataframe\ndf_restaurant.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Verify the dtypes of the restaurants dataframe\ndf_restaurant.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Review the head of the dataframe to make sure it looks as expected\ndf_restaurant.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Describe the score to see if there is nuch variance in the values\ndf_restaurant.score.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# How many of the top 30 sites / venues had > 10 restaurants nearby\ndf_restaurant.venue_name.nunique()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# How many unique restaurant categories are there\ndf_restaurant.category.nunique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# How many unique restaurants are there\ndf_restaurant.name.nunique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# What arethe top 10 most frequently occuring restaurant types\ndf_restaurant.groupby('category')['name'].count().sort_values(ascending=False)[:10]\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Which restaurants have to highest average score\ndf_restaurant.groupby('category')['score'].mean().sort_values(ascending=False)[:10]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Finally write the dataframe to a pickle file for restoring later\ndf_restaurant.to_pickle('./capstone_pickles/restaurants.pkl')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Import and process the Chicago Crime DataSet\nThis dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago in the last year, minus the most recent seven days. Data is extracted from the Chicago Police Department's CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified.\n\nColumn Name\tType\tDescription\nCASE#\tPlain Text\tThe Chicago Police Department RD Number (Records Division Number), which is unique to the incident.\nDATE OF OCCURRENCE\tDate & Time\tDate when the incident occurred. this is sometimes a best estimate.\nBLOCK\tPlain Text\tThe partially redacted address where the incident occurred, placing it on the same block as the actual address.\nIUCR\tPlain Text\tThe Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.\nPRIMARY DESCRIPTION\tPlain Text\tThe primary description of the IUCR code.\nSECONDARY DESCRIPTION\tPlain Text\tThe secondary description of the IUCR code, a subcategory of the primary description.\nLOCATION DESCRIPTION\tPlain Text\tDescription of the location where the incident occurred.\nARREST\tPlain Text\tIndicates whether an arrest was made.\nDOMESTIC\tPlain Text\tIndicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\nBEAT\tPlain Text\tIndicates the beat where the incident occurred. A beat is the smallest police geographic area \u2013 each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.\nWARD\tNumber\tThe ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.\nFBI CD\tPlain Text\tIndicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\nX COORDINATE\tPlain Text\tThe x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\nY COORDINATE\tPlain Text\tThe y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\nLATITUDE\tNumber\tThe latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\nLONGITUDE\tNumber\tThe longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\nLOCATION\tLocation\tThe location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block.\nImport the 2018 DataSet\nThe full dataset, 2001 to 2018 contains over 6.7M rows. This makes processing the dataset difficult and time consuming. Only the 2018 data, 187222 records, will be used. In addition, the following clean-up steps are required:\n\nNot all of the columns are required. The following columns are removed:\nIUCR\nARREST\nDOMESTIC\nBEAT\nWARD\nFBI CD\nX COORDINATE\nY COORDINATE\nLOCATION"}, {"metadata": {}, "cell_type": "code", "source": "# These are the columns that we want to keep.\n# Columns not listed here won't be imported, speeding things up.\ncrime_keep_columns = ['CASE#',\n                      'DATE  OF OCCURRENCE',\n                      'BLOCK', \n                      ' PRIMARY DESCRIPTION',\n                      'WARD',\n                      'LATITUDE',\n                      'LONGITUDE']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Download csv\n# !wget -O './capstone_data/crimes.csv' https://data.cityofchicago.org/api/views/x2n5-8w5q/rows.csv?accessType=DOWNLOAD\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Read cvs download into datafram\ndf = pd.read_csv('./capstone_data/crimes.csv',\n                 usecols=crime_keep_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.shape\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Looking at the Shape of df we can see that only 7 columns have been imported instead of all 22.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "df.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.tail()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nClean up the data and prepare\nThe sample data file was downloaded in the middle of September. So half of September of 2017 was missing and half of September 2018 was not yet populated. To simply resolve this the raw data was altered to move the September 2017 dates to September 2018.\n\nNow that the data has been imported it needs to be cleaned.\n\nMove September 2017 dates to September 2018\nClean up the column names:\nStrip leading & trailing whitespace\nReplace multiple spaces with a single space\nRemove # characters\nReplace spaces with _\nConvert to lowercase\nChange the date of occurance field to a date / time object\nAdd new columns for:\nHour\nDay\nMonth\nYear\netc.\nSplit Block into zip_code and street\nVerify that all rows have valid data\n"}, {"metadata": {}, "cell_type": "code", "source": "# Strip leading & trailing whitespace\ndf.columns = df.columns.str.strip()\n\n# Replace multiple spaces with a single space\ndf.columns = df.columns.str.replace('\\s{2,}', ' ')\n\n# Replace # with blank\ndf.columns = df.columns.str.replace('#', '')\n\n# Replace spaces with _\ndf.columns = df.columns.str.replace(' ', '_')\n\n# Convert to lowercase\ndf.columns = df.columns.str.lower()\n\n# Move September 2017 dates to September 2018\ndf.date_of_occurrence.replace(to_replace=\"(09/\\\\d+)/2017\", value=r\"\\1/2018\", regex=True, inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Verify that all datatype are as expected\ndf.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\ndf['date_of_occurrence'] =  pd.to_datetime(df['date_of_occurrence'], format='%m/%d/%Y %I:%M:%S %p')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Add new columns to the dataframe to allow hourly, daily & monthly analysis\ndf['hour'] = df['date_of_occurrence'].dt.hour\ndf['day_name'] = df['date_of_occurrence'].dt.day_name()\ndf['day'] = df['date_of_occurrence'].dt.dayofweek + 1\ndf['month_name'] = df['date_of_occurrence'].dt.month_name()\ndf['month'] = df['date_of_occurrence'].dt.month\ndf['year'] = df['date_of_occurrence'].dt.year\ndf['year_month'] = df['date_of_occurrence'].dt.to_period('M')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Add the zip and street attributes\ndf['zip'] = df.block.str.split(' ').str[0]\ndf['street'] = df.block.str.split(' ').str[1:].apply(', '.join)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Verify that all rows have valid data\ndf.isna().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Drop rows with missing values \ndf.dropna(inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": " Reindex\ndf.reset_index(inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Have a final look at the crime dataframe\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": ":\n# Finally write the dataframe to a pickle file for restoring later\ndf.to_pickle('./capstone_pickles/crimes.pkl')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The data is now ready for visualisation."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}